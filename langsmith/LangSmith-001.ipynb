{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e961592-1bad-4213-bdb1-162b8fc1892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env')\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Demo LangSmith 01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f761c59-9e5d-4a8d-80c8-b1ac5edb48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Você é um assistente útil. Responda à solicitação do usuário com base apenas no contexto fornecido\"),\n",
    "        (\"user\",\"Pergunta:{question}\\nContexto:{context}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bba4d87-f465-4757-a68d-82ab7daa152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dac7847-2d4a-4923-9dc2-8216117d101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model|output_parser\n",
    "question=\"Você pode resumir esse texto?\"\n",
    "context=\"\"\" Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o pacote da OpenAI, como eu já fiz em outros vídeos. O próximo passo aqui é importar os pacotes Python que vão ser necessários. Então, além de importar o pacote OpenAI, vou precisar do pandas e numpy. Aqui eu preciso configurar a chave de API da OpenAI, vou gerar uma chave de API nova, assim você pode relembrar onde e como fazer. Pra garantir uma compreensão muito clara desse conceito e como ele funciona, vou começar com uma lista muito simples de palavras. Esse é o arquivo CSV de palavras. Aqui temos algumas palavras como batata frita, refrigerante, hambúrguer, banana, café e coisas assim. Bom, agora vou carregar esse arquivo CSV em um dataframe do pandas e vou chamar esse dataframe de idf. A próxima coisa que faremos é calcular os Embeddings das palavras, mas o que é isso? Isso significa converter essas palavras em vetores. E então, o que eu vou fazer aqui é usar a OpenAI pra essa tarefa. E se você olhar na documentação da OpenAI, tem uma seção inteira sobre Embeddings, com mais detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição pra API com uma palavra, como retorno eu recebo esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API, dessa forma, o que podemos fazer com o pacote Python da OpenAI é importar uma função chamada getEmbedding. Dessa forma eu só preciso passar uma string de texto, chamar a função getEmbedding e escolher uma engine, ou seja, um modelo. Como opção, temos o DaVinci, Ada, Cori, mas pra esses Embeddings de texto, vamos usar o modelo ada002. Então vou digitar a palavra caputino, e ao executar isso, a função está passando a string e convertendo essa palavra caputino pra um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores pra todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez, e criar uma nova coluna, com todos os vetores bem ao lado. Então nesse caso, posso fazer isso em lotes. Vou pegar o DataFrame e criar uma nova coluna chamada Embedding, e dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso eu utilizo a função apply, que itere em cada elemento da coluna text, e vai aplicar uma função lambda a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa uma única expressão. Nesse caso, a função getEmbedding. De maneira simples, essa linha de código aplica a função getEmbedding a cada elemento da coluna text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal Daniel, mas o que eu faço com eles? Bem, essa é a parte que a coisa fica interessante. Vou utilizar como termo de pesquisa a palavra Caputino, que geramos em Embedding inicialmente. Só lembrando que Caputino não aparece nessa lista de palavras. Então com esse vetor numérico, vou procurar uma semelhança nas representações numéricas de todas essas outras palavras que temos armazenadas no DataFrame. Ou seja, eu quero descobrir quais vetores estão mais próximos do vetor da palavra Caputino. Em outras palavras, vamos fazer uma pesquisa por similaridade. E pra fazer isso, o modelo faz um cálculo da medida de similaridade entre dois vetores, chamada cosine similarity, ou similaridade do cosseno. Essa é uma fórmula matemática que calcula a similaridade entre duas sequências de números, e é descrita por essa equação. A similaridade de cosseno é obtida dividindo o produto escalado dos dois vetores pelas magnitudes desses vetores. Essa anotação pode parecer complicada ou muito simples, dependendo do seu nível de matemática, mas eu garanto que é apenas multiplicação. Bom, explicando de forma bem simples, esse termo superior aqui é um produto escalar, que é obtido somando os produtos dos elementos correspondentes dos vetores. Traduzindo, o objetivo é apenas multiplicar cada um dos termos nos dois vetores juntos. Suponha que você tenha essa equação aqui com dois vetores, onde o primeiro vetor, v1, são os números 1, 2 e 3, e o segundo vetor, v2, é 4, 5 e 6. Então o cálculo ficaria 1 vezes 4, mais 2 vezes 5, mais 3 vezes 6. E como resultado, temos o produto escalar. Ou, em Python, podemos usar NumPy para fazer isso, obtendo o mesmo resultado, 32. E então, na parte inferior da equação, precisamos calcular a magnitude. A magnitude de um vetor é um valor escalar que representa o tamanho ou o comprimento do vetor. Ele é calculado como a raiz quadrada da soma dos quadrados dos elementos do vetor. Ou seja, 1 ao quadrado, mais 2 ao quadrado, mais 3 ao quadrado, que nesse caso dá 14. Eu pego a raiz quadrada disso, ou seja, 3.74. Faço essa mesma conta no v2, multiplico os dois, e tenho a magnitude. Agora só preciso dividir o produto escalar pela magnitude para obter a similaridade, que nesse caso é de 0.97, entre essas duas sequências de números. No final, tudo é apenas uma soma de várias multiplicações. Você pode até pegar esses números aqui e plotar em um vetor 3D. Se eu colocar 1, 2 e 3 aqui como um vetor 4, 5 e 6 e desenhar, você pode ver que eles estão bem próximos. Voltando às palavras, o que estamos fazendo aqui é pegando uma representação vetorial de todas essas palavras diferentes e ver quão próximas elas estão no espaço. Claro que é difícil realizar esse cálculo manualmente com essa quantidade de números, mas felizmente existe uma função integrada na própria PNEI, chamada Cosine Similarity, que percorrerá todos esses números e executará esse cálculo. Então agora podemos simplesmente classificar todos esses diferentes vetores no espaço e descobrir qual vetor está mais próximo de Caputino. O que vamos fazer aqui é pegar essa função de similaridade do cosseno e aplicá-la a cada vetor que temos no nosso dataframe. Ou seja, vamos verificar a distância entre cada um desses vetores e o vetor para Caputino. Assim executamos esse cálculo e armazenamos o resultado em uma nova coluna chamada Similaridades. Por fim, ordenamos em ordem decrescente pela coluna Similaridades. Como resultado, temos as palavras mais similares a Caputino. Nesse caso, Expresso, Mocha, Latte e Café, o que faz total sentido. Se você ficou interessado nesse assunto e quer saber mais sobre como emberes funcionam e como esses modelos entendem o significado de palavras, deixe aí nos comentários. Agora pense o seguinte, você está desenvolvendo um assistente inteligente, similar ao chat GPT, com foco em um domínio específico, dentro de um negócio. Pois bem, esses modelos precisam saber o máximo possível sobre as suas operações, mas a maioria das empresas possui tanta informação que colocar tudo dentro de um só contexto em uma única solicitação é inviável. Um outro problema é que nas conversas, os usuários geralmente não fazem perguntas baseadas em palavras-chave específicas, como fazem no Google. Em vez disso, fazem perguntas que expressam um significado ou uma intenção específica. Ou seja, algoritmos de busca tradicionais teriam dificuldades com esse tipo de consulta. E é aqui que soluções como o Pinecone entram em cena. Imagina poder fazer emberens de frases ou até parágrafos inteiros e armazenar isso em um banco de dados. Bancos de dados de vetores ou vector databases permitem que você expanda de maneira absurda a base de conhecimento do seu chatbot com os seus dados personalizados. Vamos ver isso na prática. Nesse exemplo, vou usar como base de conhecimento essa portilha aberta de Python da Kaelo. A ideia é vetorizar todo esse conteúdo para fazer consultas por similaridade e utilizar o resultado como contexto nos prompts. Depois de importar os pacotes necessários, preciso garantir que o documento não exceda o limite de tokens. Para isso, a estratégia é dividir o documento em partes menores. É uma boa prática dividir os documentos em partes semanticamente relevantes. E geralmente, os parágrafos funcionam bem. Nesse exemplo, cada parágrafo vai ser utilizado para gerar uma embedding. No script, primeiro percorro a pasta onde salvei o documento e leio o apostilo. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente removo as partes que tenham menos de 10 palavras, pois é improvável que forneçam informações úteis para um contexto. Agora os embeddings podem ser gerados usando o modelo textEmbeddingAda002. Feito isso, o próximo passo é carregar os dados, neste caso, os vetores no Pinecone. Depois de configurado o acesso ao API do Pinecone, posso criar o índice e conectar a base. Finalmente, faço o upload dos dados para o índice em lotes de 64. Para cada lote, é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks, ou as partes que foram separadas em parágrafos e deram origem aos embeddings. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para upload, posso simplesmente chamar o método indexUpsearch com os vetores que foram definidos. E depois que os dados forem carregados, posso realizar algumas perguntas de exemplo, para encontrar os documentos mais relevantes para essa pergunta. Antes é preciso fazer o embedding da pergunta em si. Em seguida, posso realizar uma pesquisa por similaridade, que compara o embedding da pergunta com todos os outros vetores no índice do Pinecone. O parâmetro topKey define quantos resultados próximos serão retornados, e o parâmetro includeMetadata é necessário para obter nosso texto de volta com a resposta. Agora que o teste de consulta foi feito, vamos seguir para a parte final. O último passo é usar esses resultados como contexto em um prompt, nas solicitações para o modelo GPT. Aqui um bom ponto de partida para o prompt é informar que você espera respostas diretas sobre o contexto, e caso a resposta não esteja presente no contexto, o modelo deve dizer apenas eu não sei. Dessa forma estamos impedindo o modelo de alucinar. Isso ocorre quando ele traz respostas não relacionadas à pergunta original. E como esperado, o resultado é bastante satisfatório. Se compararmos com o conteúdo da apostila, veremos que o texto foi utilizado com sucesso. Para concluir, meu palpite é que esse vai ser o caminho das aplicações que utilizarem LLMs como chat GPT. Projetos com uma base de conhecimento sobre domínios específicos, utilizando o Vector Database como pinecone, seguido de consultas por similaridade e prompts bem projetados. Por hoje é só, eu vou ficando por aqui e a gente se vê no próximo vídeo. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de599118-bde1-4c69-b2c6-e46bd4186a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings são representações numéricas de palavras que preservam similaridades de significado, permitindo a construção de mecanismos de busca semânticos. O vídeo explora como usar Embeddings da OpenAI para criar um sistema de busca, começando pela instalação do pacote da OpenAI e a importação de bibliotecas necessárias. O processo envolve a conversão de palavras em vetores numéricos, calculando a similaridade entre eles usando a similaridade do cosseno. Após gerar vetores para uma lista de palavras, o sistema busca palavras semelhantes a uma entrada, como \"caputino\". O vídeo também menciona a aplicação de Embeddings em bases de dados vetoriais, como o Pinecone, para melhorar a busca em assistentes inteligentes, permitindo consultas por similaridade em textos maiores. A conclusão aponta para o futuro das aplicações de LLMs utilizando bases de conhecimento específicas e consultas otimizadas.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\":question,\"context\":context}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81876a-16bc-4b57-8772-f3ee83f978d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
