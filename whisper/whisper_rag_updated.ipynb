{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f497904-26e5-4f48-b6f0-04198278a1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf48350-e6a6-4154-b8dd-37fd061183ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    filename = 'infoslack.mp4'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=iw2TeYESnTk\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7cbdb-312f-4dd1-ba9a-7097687d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio(url):\n",
    "    \n",
    "    filename = 'infoslack-audio'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio',\n",
    "        'postprocessors':[{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '128',\n",
    "        }],\n",
    "        'outtmpl': filename,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "        \n",
    "download_audio(\"https://www.youtube.com/watch?v=iw2TeYESnTk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b271f381-784a-4be8-8c15-e25163219cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 13s, sys: 1min 31s, total: 7min 44s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper local\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"infoslack-audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38496097-9ee6-4c65-b073-02e8018ce101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O orden bearings são a forma de representar palavras e textos como vetores numericos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converse uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como mecanismo de busca, com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semânticos usando emberes da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, convertem númeres para realizar a classificação, da textão de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural. Bem, eu preparei esse exemplo sobre emberes de palavras e agora vamos ver como tudo isso funciona. A matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o pacote da OpenAI, como já fiz em outros vídeos. O próximo passo aqui é importar os pacotes Python que vão ser necessários. Então, além de importar o pacote da OpenAI, vou precisar do Pandas e NumPy. Aqui, eu preciso configurar a chave de API da OpenAI. Vou gerar um chave de API nova, assim você pode relembrar onde como fazer. Para garantir uma compreensão muito clara desse conceito e como ele funciona, vou começar com uma lista muito simples de palavras. Esse é o arquivo CSV de palavras. Temos algumas palavras como Batata Frita, refrigerante, hamburgues, banana, café e coisas assim. Agora, vou carregar esse arquivo CSV em um Datafream do Pandas e vou chamar esse Datafream de IDF. A próxima coisa que faremos é calcular os emberes das palavras. Mas, o que é isso? Isso significa converter essas palavras em vetores. E então, o que eu vou fazer aqui é usar o API para essa tarefa. E, se você olhar na documentação da OpenAI, tem uma sessão inteira sobre emberes com mais detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição para API com uma palavra, como retorno o recebo, esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API dessa forma, o que podemos fazer com o pacote Python da OpenAI, importar uma função chamada GetInBerry. Dessa forma, eu só preciso passar um string de texto, chamar a função GetInBerry e escolher o Engine, ou seja, um modelo. Com uma opção, temos o da 20, aada, currê, mais pra esses emberes de texto. Vamos usar um modelo a 002. Então, vou digitar a palavra caputino. E, ao executar isso, a função está passando a string e convertendo essa palavra caputino para um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores para todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez. E criar uma nova coluna, com todos os vetores bem ao lado. Então, nesse caso, posso fazer isso em lots. Vou pegar o DataFrame e criar uma nova coluna chamada em Berry. E, dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso, utiliza a função Apply, queitere em cada elemento da coluna Text, e vai aplicar uma função Lambda, a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa um a única expressão, nesse caso, a função GetInBerry. De maneira simples, essa linha de código aplica a função GetInBerry, a cada elemento da coluna Text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal, Daniel, mas o que eu faço com eles? Bem, essa é a parte que é coisa que fique interessante. Vou utilizar como termo de pesquisa a palavra caputino, que geramos em Berry inicialmente. Só lembrando que caputino não aparece nessa lista de palavras. Então, com esse vetor numérico, vou procurar uma semelhança. Nas representações numéricas de todas essas outras palavras que temos armazenadas no DataFrame. Ou seja, eu quero descobrir quais vetores estão mais próximos do vetor da palavra caputino. Em outras palavras, vamos fazer uma pesquisa por similaridade. E pra fazer isso, o modelo faz um cálculo da medida de similaridade entre dois vetores, chamada Cossine Similarity, ou similaridade do Cosseno. Essa é uma fórmula matemática que calcula a similaridade entre duas sequências de números, e é descrita por essa coação. A similaridade de Cosseno é o vitil da dividida no produto escalado dos dois vetores pelas magnitudes desses vetores. Essa notação pode parecer complicada, ou muito simples, dependendo do seu nível de matemática, mas eu garanto que é apenas multiplicação. Bom, explicando de forma bem simples esse termo superior aqui, é um produto escalar, que é o vitido somando os produtos dos elementos correspondentes dos vetores. Traduzindo, o objetivo é apenas multiplicar cada um dos termos nos dois vetores juntos. Suponha que você tem essa equação aqui com dois vetores, onde o primeiro vetor V1 são os números 1, 2 e 3. E o segundo vetor, ou V2, é 4, 5 e 6. Então, o cálculo ficaria 1 vezes 4 mais 2 vezes 5 mais 3 vezes 6. E como resultado, temos o produto escalar. Ou em Python, podemos usar ou não para fazer isso. Obtendo o mesmo resultado, 32. E então, na parte inferior da equação, precisamos calcular a magnitude. A magnitude de um vetor é um valor escalar que representa o tamanho ou o comprimento do vetor. É calculado como a raiz quadrada da soma dos quadrados dos elementos do vetor. Ou seja, 1 ao quadrado, mais 2 ao quadrado, mais 3 ao quadrado, que nesse caso dá 14. Eu pego a raiz quadrada disso, ou seja, 3.74. Faça essa mesma conta no V2, multiplique os dois, e tem a magnitude. Agora, eu só preciso dividir o produto escalar pela magnitude para obter a similaridade. Que nesse caso, é de 0.97, entre essas duas sequências de números. No final, tudo é apenas uma soma de várias multiplicações. Se pode até pegar esses números aqui, plotar em um vetor 3D. Se eu colocar 1, 2 e 3 aqui, como um vetor 4, 5, 6, e desenhar, você pode ver que eles estão bem próximos. Voltando às palavras, o que estamos fazendo aqui, é pegando uma representação vetorial de todas essas palavras diferentes e ver qual próximas elas estão no espaço. Claro que é difícil realizar esse cálculo manualmente com essa quantidade de números, mas, felizmente, existe uma função integrada na própria OPNI, chamada a coçade simulária, que percorrerá todos esses números e executará esse cálculo. Então, agora podemos simplesmente classificar todos esses diferentes vetores no espaço e descobrir qual o vetor está mais próximo de capotino. Que vamos fazer aqui, é pegar essa função de simulária e idade do coçeno e aplicá-la a cada vetor que temos no nosso data frame. Ou seja, vamos verificar a distância entre cada um desses vetores e o vetor para capotino. Em tensa que o estamos nesse cálculo e armazenamos o resultado em uma nova coluna chamada a simularidades. Por fim, ordenamos em ordem de acrescente pela coluna simularidades. Como resultado, temos as palavras mais similares a capotina. Nesse caso, expresso o Moca Laté e Café, o que faz total sentido. Se você ficou interessado nesse assunto e quer saber mais sobre como embarees funcionam e como esses modelos entendem o significado de palavras, deixa aí nos comentários. Agora penso o seguinte, você está desenvolvendo um assistente inteligente simular o chat gpt com foco em um domínio específico dentro de um negócio. Pois bem, esses modelos precisam saber o máximo possível sobre as suas operações. Mas a maioria das empresas possui tanta informação que coloca tudo dentro de um só contexto, em uma única só licitação enviável. Um outro problema é que nas conversas, os usuários geralmente não fazem perguntas baseadas em palavras chave, específicas como fazem no Google. Em vez disso, fazem perguntas que expressam um significado ou uma intenção específica. Ou seja, algoritmos de busca tradicional estariam dificuldades com esse tipo de consulta. E aqui, que soluções como painicone entram em cena. Imagina poder fazer embarees de frases ou até paraagra-vos inteiros. De armazenar isso em um banco de dados. Banco de dados de vetores ou vector databases Permite que você expanda de maneira absurda a base de conhecimento do seu chatbot com os seus dados personalizados. Vamos ver isso na prática. Nesse exemplo, vou usar como base de conhecimento essa pochila aberta de Python da caelo. Aí deve autorizar todo esse conteúdo para fazer consultas por similaridade e utilizar o resultado como contexto nos prontes. Depois de importar os pacotes necessários, preciso garantir que o documento não é cedo limite de tokens. Pra isso, a estratégia é dividido o documento em partes menores. É uma boa prática de vídeos documentos em partes semanticamente relevantes. E geralmente, os parágrafos funcionam bem. Nesse exemplo, cada parágrafo vai ser utilizado para gerar uma embaring. No script, primeiro percorro a parte da onde salver o documento e leu a pochila. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente, removo as partes que tenham menos de 10 palavras, pois, improvável que forneça informações úteis para um contexto. Agora os embérios podem ser gerados usando o modelo Text Embaring Addas 002. Feito isso, o próximo passo é carregar os lados. Nesse caso, os vetores no panicônia. Depois de configurado o acesso a API do panicônia, possa criar o índice e conectar a base. Finalmente, faço o apilô de dos dados pro índice em lots de 64. Para cada lote é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks ou as partes que foram separadas em parágrafos e deram origem aos embérios. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para o pelot, possa simplesmente chamar o método index absurd com os vetores que foram definidos. E depois que os dados forem carregados, posso realizar algumas perguntas de exemplo. Para encontrar os documentos mais relevantes para essa pergunta. Antes de fazer o embaring, dá pergunta em si. Nesse seguida, posso realizar uma pesquisa possibilidade que comparam o embaring da pergunta com todos os outros vetores no índice do panicón. O parâmetro topkey define quantos resultados próximos serão retornados. E o parâmetro, inclui o métodeita, é necessário para obter nosso texto e volta com a resposta. Agora que o teste de consulta foi feito, vamos seguir para a parte final. O último passo é usar esses resultados como contexto em um prąpe. Nae sólos citações para o modelo GPT. Aqui é um bom ponto de partida para o prąpe, é informar que você espera respostas diretas sobre o contexto e caso a resposta não esteja presente no contexto, o modelo deve dizer apenas, eu não sei. Deça forma estamos impedindo o modelo de alucinar. Isso ocorre quando ele traz respostas não relacionadas a perguntar original. E como esperado, o resultado é bastante satisfatório. Se compararmos com o conteúdo da apostil, veremos que o texto foi utilizado com sucesso. Para concluir, meu palpite o que esse vai ser o caminho das aplicações que utilizarem a Lele EMS como chatRPT. Projetos com uma base de conhecimento sobre domínios específicos utilizando o Vector Data Base como panicónio. Seguido de consultas por similaridade e prąpe de bem projetados. Por hoje é só, eu vou ficando por aqui e a gente se vê no próximo vídeo.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6687192-b34c-4de1-888d-d00f5f59453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 ms, sys: 73.6 ms, total: 286 ms\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper API\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"infoslack-audio.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27549fef-2001-4a38-953a-4501454544b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text.txt', 'w') as file:\n",
    "    file.write(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea540f1-ed31-4751-8e12-fafefc9642c9",
   "metadata": {},
   "source": [
    "#### output openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf05af8-3e82-42b4-8d94-f09cc28f91a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o pacote da OpenAI, como eu já fiz em outros vídeos. O próximo passo aqui é importar os pacotes Python que vão ser necessários. Então, além de importar o pacote OpenAI, vou precisar do pandas e numpy. Aqui eu preciso configurar a chave de API da OpenAI, vou gerar uma chave de API nova, assim você pode relembrar onde e como fazer. Pra garantir uma compreensão muito clara desse conceito e como ele funciona, vou começar com uma lista muito simples de palavras. Esse é o arquivo CSV de palavras. Aqui temos algumas palavras como batata frita, refrigerante, hambúrguer, banana, café e coisas assim. Bom, agora vou carregar esse arquivo CSV em um dataframe do pandas e vou chamar esse dataframe de idf. A próxima coisa que faremos é calcular os Embeddings das palavras, mas o que é isso? Isso significa converter essas palavras em vetores. E então, o que eu vou fazer aqui é usar a OpenAI pra essa tarefa. E se você olhar na documentação da OpenAI, tem uma seção inteira sobre Embeddings, com mais detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição pra API com uma palavra, como retorno eu recebo esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API, dessa forma, o que podemos fazer com o pacote Python da OpenAI é importar uma função chamada getEmbedding. Dessa forma eu só preciso passar uma string de texto, chamar a função getEmbedding e escolher uma engine, ou seja, um modelo. Como opção, temos o DaVinci, Ada, Cori, mas pra esses Embeddings de texto, vamos usar o modelo ada002. Então vou digitar a palavra caputino, e ao executar isso, a função está passando a string e convertendo essa palavra caputino pra um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores pra todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez, e criar uma nova coluna, com todos os vetores bem ao lado. Então nesse caso, posso fazer isso em lotes. Vou pegar o DataFrame e criar uma nova coluna chamada Embedding, e dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso eu utilizo a função apply, que itere em cada elemento da coluna text, e vai aplicar uma função lambda a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa uma única expressão. Nesse caso, a função getEmbedding. De maneira simples, essa linha de código aplica a função getEmbedding a cada elemento da coluna text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal Daniel, mas o que eu faço com eles? Bem, essa é a parte que a coisa fica interessante. Vou utilizar como termo de pesquisa a palavra Caputino, que geramos em Embedding inicialmente. Só lembrando que Caputino não aparece nessa lista de palavras. Então com esse vetor numérico, vou procurar uma semelhança nas representações numéricas de todas essas outras palavras que temos armazenadas no DataFrame. Ou seja, eu quero descobrir quais vetores estão mais próximos do vetor da palavra Caputino. Em outras palavras, vamos fazer uma pesquisa por similaridade. E pra fazer isso, o modelo faz um cálculo da medida de similaridade entre dois vetores, chamada cosine similarity, ou similaridade do cosseno. Essa é uma fórmula matemática que calcula a similaridade entre duas sequências de números, e é descrita por essa equação. A similaridade de cosseno é obtida dividindo o produto escalado dos dois vetores pelas magnitudes desses vetores. Essa anotação pode parecer complicada ou muito simples, dependendo do seu nível de matemática, mas eu garanto que é apenas multiplicação. Bom, explicando de forma bem simples, esse termo superior aqui é um produto escalar, que é obtido somando os produtos dos elementos correspondentes dos vetores. Traduzindo, o objetivo é apenas multiplicar cada um dos termos nos dois vetores juntos. Suponha que você tenha essa equação aqui com dois vetores, onde o primeiro vetor, v1, são os números 1, 2 e 3, e o segundo vetor, v2, é 4, 5 e 6. Então o cálculo ficaria 1 vezes 4, mais 2 vezes 5, mais 3 vezes 6. E como resultado, temos o produto escalar. Ou, em Python, podemos usar NumPy para fazer isso, obtendo o mesmo resultado, 32. E então, na parte inferior da equação, precisamos calcular a magnitude. A magnitude de um vetor é um valor escalar que representa o tamanho ou o comprimento do vetor. Ele é calculado como a raiz quadrada da soma dos quadrados dos elementos do vetor. Ou seja, 1 ao quadrado, mais 2 ao quadrado, mais 3 ao quadrado, que nesse caso dá 14. Eu pego a raiz quadrada disso, ou seja, 3.74. Faço essa mesma conta no v2, multiplico os dois, e tenho a magnitude. Agora só preciso dividir o produto escalar pela magnitude para obter a similaridade, que nesse caso é de 0.97, entre essas duas sequências de números. No final, tudo é apenas uma soma de várias multiplicações. Você pode até pegar esses números aqui e plotar em um vetor 3D. Se eu colocar 1, 2 e 3 aqui como um vetor 4, 5 e 6 e desenhar, você pode ver que eles estão bem próximos. Voltando às palavras, o que estamos fazendo aqui é pegando uma representação vetorial de todas essas palavras diferentes e ver quão próximas elas estão no espaço. Claro que é difícil realizar esse cálculo manualmente com essa quantidade de números, mas felizmente existe uma função integrada na própria PNEI, chamada Cosine Similarity, que percorrerá todos esses números e executará esse cálculo. Então agora podemos simplesmente classificar todos esses diferentes vetores no espaço e descobrir qual vetor está mais próximo de Caputino. O que vamos fazer aqui é pegar essa função de similaridade do cosseno e aplicá-la a cada vetor que temos no nosso dataframe. Ou seja, vamos verificar a distância entre cada um desses vetores e o vetor para Caputino. Assim executamos esse cálculo e armazenamos o resultado em uma nova coluna chamada Similaridades. Por fim, ordenamos em ordem decrescente pela coluna Similaridades. Como resultado, temos as palavras mais similares a Caputino. Nesse caso, Expresso, Mocha, Latte e Café, o que faz total sentido. Se você ficou interessado nesse assunto e quer saber mais sobre como emberes funcionam e como esses modelos entendem o significado de palavras, deixe aí nos comentários. Agora pense o seguinte, você está desenvolvendo um assistente inteligente, similar ao chat GPT, com foco em um domínio específico, dentro de um negócio. Pois bem, esses modelos precisam saber o máximo possível sobre as suas operações, mas a maioria das empresas possui tanta informação que colocar tudo dentro de um só contexto em uma única solicitação é inviável. Um outro problema é que nas conversas, os usuários geralmente não fazem perguntas baseadas em palavras-chave específicas, como fazem no Google. Em vez disso, fazem perguntas que expressam um significado ou uma intenção específica. Ou seja, algoritmos de busca tradicionais teriam dificuldades com esse tipo de consulta. E é aqui que soluções como o Pinecone entram em cena. Imagina poder fazer emberens de frases ou até parágrafos inteiros e armazenar isso em um banco de dados. Bancos de dados de vetores ou vector databases permitem que você expanda de maneira absurda a base de conhecimento do seu chatbot com os seus dados personalizados. Vamos ver isso na prática. Nesse exemplo, vou usar como base de conhecimento essa portilha aberta de Python da Kaelo. A ideia é vetorizar todo esse conteúdo para fazer consultas por similaridade e utilizar o resultado como contexto nos prompts. Depois de importar os pacotes necessários, preciso garantir que o documento não exceda o limite de tokens. Para isso, a estratégia é dividir o documento em partes menores. É uma boa prática dividir os documentos em partes semanticamente relevantes. E geralmente, os parágrafos funcionam bem. Nesse exemplo, cada parágrafo vai ser utilizado para gerar uma embedding. No script, primeiro percorro a pasta onde salvei o documento e leio o apostilo. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente removo as partes que tenham menos de 10 palavras, pois é improvável que forneçam informações úteis para um contexto. Agora os embeddings podem ser gerados usando o modelo textEmbeddingAda002. Feito isso, o próximo passo é carregar os dados, neste caso, os vetores no Pinecone. Depois de configurado o acesso ao API do Pinecone, posso criar o índice e conectar a base. Finalmente, faço o upload dos dados para o índice em lotes de 64. Para cada lote, é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks, ou as partes que foram separadas em parágrafos e deram origem aos embeddings. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para upload, posso simplesmente chamar o método indexUpsearch com os vetores que foram definidos. E depois que os dados forem carregados, posso realizar algumas perguntas de exemplo, para encontrar os documentos mais relevantes para essa pergunta. Antes é preciso fazer o embedding da pergunta em si. Em seguida, posso realizar uma pesquisa por similaridade, que compara o embedding da pergunta com todos os outros vetores no índice do Pinecone. O parâmetro topKey define quantos resultados próximos serão retornados, e o parâmetro includeMetadata é necessário para obter nosso texto de volta com a resposta. Agora que o teste de consulta foi feito, vamos seguir para a parte final. O último passo é usar esses resultados como contexto em um prompt, nas solicitações para o modelo GPT. Aqui um bom ponto de partida para o prompt é informar que você espera respostas diretas sobre o contexto, e caso a resposta não esteja presente no contexto, o modelo deve dizer apenas eu não sei. Dessa forma estamos impedindo o modelo de alucinar. Isso ocorre quando ele traz respostas não relacionadas à pergunta original. E como esperado, o resultado é bastante satisfatório. Se compararmos com o conteúdo da apostila, veremos que o texto foi utilizado com sucesso. Para concluir, meu palpite é que esse vai ser o caminho das aplicações que utilizarem LLMs como chat GPT. Projetos com uma base de conhecimento sobre domínios específicos, utilizando o Vector Database como pinecone, seguido de consultas por similaridade e prompts bem projetados. Por hoje é só, eu vou ficando por aqui e a gente se vê no próximo vídeo.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5489ad-7548-4b04-b033-9a58acebb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 141 ms, sys: 64.5 ms, total: 205 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper API Groq\n",
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "resp = client.audio.transcriptions.create(\n",
    "    model=\"whisper-large-v3\",\n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27326d4-fc4b-4703-8a62-88403d8b03cd",
   "metadata": {},
   "source": [
    "#### output groq api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fdfc88-2944-41f7-80a7-2c0f6253b878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Word Embeddings são a forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar a classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural. Bem, eu preparei esse exemplo sobre embeddings de palavras, e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o pacote da OpenAI, como já fiz em outros vídeos. O próximo passo aqui é importar os pacotes Python que vão ser necessários. Então além de importar o pacote OpenAI, vou precisar do Pandas e NumPy. Aqui eu preciso configurar a chave de API da OpenAI, vou gerar uma chave de API nova, assim você pode relembrar onde e como fazer. Para garantir uma compreensão muito clara desse conceito e como ele funciona, vou começar com uma lista muito simples de palavras. Esse é o arquivo CSV de palavras. Temos algumas palavras como batata frita, refrigerante, hambúrguer, banana, café e coisas assim. Bom, agora eu vou carregar esse arquivo CSV em um dataframe do Pandas e vou chamar esse dataframe de IDF. A próxima coisa que faremos é calcular os embeddings das palavras. Mas o que é isso? Isso significa converter essas palavras em vetores. E então o que eu vou fazer aqui é usar o OpenAI para essa tarefa. E se você olhar na documentação do OpenAI, tem uma seção inteira sobre embeddings com mais detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição para API com uma palavra, como retorno eu recebo esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API dessa forma, o que podemos fazer com o pacote Python da OpenAI é importar uma função chamada getEmbedding. Dessa forma eu só preciso passar uma string de texto, chamar a função getEmbedding e escolher uma engine, ou seja, um modelo. Como opção, temos o da 20, ada, corri, mas para esses embeds de texto, vamos usar o modelo ada002. Então vou digitar a palavra caputino, e ao executar isso, a função está passando a string e convertendo essa palavra caputino para um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora a mesma coisa no dataframe gerando vetores para todas essas palavras Aqui eu tenho um total de 26 palavras e quero converter todas elas de uma vez e criar uma nova coluna com todos os vetores bem ao lado Ent nesse caso posso fazer isso em Lottes Vou pegar o DataFrame e criar uma nova coluna chamada Embedding. E dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Para isso, eu utilizo a função Apply, que itere em cada elemento da coluna Text e vai aplicar uma função Lambda a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento e executa uma única expressão. Nesse caso, a função getEmbedding. De maneira simples, essa linha de código aplica a função getEmbedding a cada elemento da coluna text. Como resultado, para cada palavra nesse data frame, tem uma grande lista de números associados. Legal, Daniel, mas o que eu faço com eles? Bem, essa é a parte que a coisa fica interessante. Vou utilizar como termo de pesquisa a palavra caputino, que geramos o embedding inicialmente. Só lembrando que caputino não aparece nessa lista de palavras. E então com esse vetor numérico vou procurar uma semelhança nas representações numéricas de todas essas outras palavras que temos armazenadas no data frame. Ou seja, eu quero descobrir quais vetores estão mais próximos do vetor da palavra caputino. Em outras palavras, vamos fazer uma pesquisa por similaridade. E para fazer isso, o modelo faz um cálculo da medida de similaridade entre dois vetores, chamada cosine similarity, ou similaridade do cosseno. Essa é uma fórmula matemática que calcula a similaridade entre duas sequências de números. e é descrita por essa equação. A similaridade de cosseno é obtida dividindo o produto escalar dos dois vetores pelas magnitudes desses vetores. Essa anotação pode parecer complicada ou muito simples, dependendo do seu nível de matemática, mas eu garanto que é apenas multiplicação. Bom, explicando de forma bem simples, esse termo superior aqui é um produto escalar, que é obtido somando os produtos dos elementos correspondentes dos vetores. Traduzindo, o objetivo é apenas multiplicar cada um dos termos nos dois vetores juntos. Suponha que você tem essa equação aqui com dois vetores Onde o primeiro vetor, v1, são os números 1, 2 e 3 E o segundo vetor, ou v2, é 4, 5 e 6 Então o cálculo ficaria 1 vezes 4 mais 2 vezes 5 mais 3 vezes 6 E como resultado temos o produto escalar Ou em Python podemos usar o numpy para fazer isso Obtendo o mesmo resultado, 32 E então na parte inferior da equação precisamos calcular a magnitude A magnitude de um vetor é um valor escalar Que representa o tamanho ou o comprimento do vetor É calculado como a raiz quadrada da soma dos quadrados dos elementos do vetor. Ou seja, 1² mais 2² mais 3², que nesse caso dá 14. Eu pego a raiz quadrada disso, ou seja, 3.74. Faço essa mesma conta no V2, multiplico os dois e tenho a magnitude. Agora s preciso dividir o produto escalar pela magnitude para obter a similaridade que nesse caso de 0 entre essas duas sequ de n No final tudo apenas uma soma de v multiplica Você pode até pegar esses números aqui e plotar em um vetor 3D. Se eu colocar 1, 2 e 3 aqui como um vetor 4, 5 e 6 e desenhar, você pode ver que eles estão bem próximos. Voltando às palavras, o que estamos fazendo aqui é pegando uma representação vetorial de todas essas palavras diferentes e ver quão próximas elas estão no espaço. Claro que é difícil realizar esse cálculo manualmente, com essa quantidade de números, mas felizmente existe uma função integrada na própria PNI, chamada cosine similarity, que percorrerá todos esses números e executará esse cálculo. Então agora podemos simplesmente classificar todos esses diferentes vetores no espaço e descobrir qual vetor está mais próximo de Caputino. O que vamos fazer aqui é pegar essa função de similaridade do cosseno e aplicá-la a cada vetor que temos no nosso data frame. Ou seja, vamos verificar a distância entre cada um desses vetores e o vetor para a caputina. Então executamos esse cálculo e armazenamos o resultado em uma nova coluna chamada Similaridades. Por fim, ordenamos em ordem decrescente pela coluna Similaridades. Como resultado, temos as palavras mais similares à caputina. Nesse caso, Expresso, Mocha, Latte e Café, o que faz total sentido. Se você ficou interessado nesse assunto e quer saber mais sobre como embeds funcionam e como esses modelos entendem o significado de palavras, deixe aí nos comentários. Agora pense o seguinte, você está desenvolvendo um assistente inteligente, similar ao chat GPT, com foco em um domínio específico, dentro de um negócio. Pois bem, esses modelos precisam saber o máximo possível sobre as suas operações, mas a maioria das empresas possui tanta informação que colocar tudo dentro de um só contexto, em uma única solicitação, é inviável. Um outro problema é que nas conversas, os usuários geralmente não fazem perguntas baseadas em palavras-chave específicas, como fazem no Google. Em vez disso, fazem perguntas que expressam um significado ou uma intenção específica. Ou seja, algoritmos de busca tradicionais teriam dificuldades com esse tipo de consulta. E é aqui que soluções como o Panicone entram em cena. Imagina poder fazer embeddings de frases ou até parágrafos inteiros e armazenar isso em um banco de dados. Bancos de dados de vetores ou vector databases permitem que você expanda de maneira absurda a base de conhecimento do seu chatbot com os seus dados personalizados. Vamos ver isso na prática. Nesse exemplo eu vou usar como base de conhecimento essa apostila aberta de Python da Kaelon. A ideia é vetorizar todo esse conteúdo para fazer consultas por similaridade e utilizar o resultado como contexto nos prompts. Depois de importar os pacotes necess preciso garantir que o documento n exceda o limite de tokens Para isso a estrat dividir o documento em partes menores uma boa pr dividir os documentos em partes semanticamente relevantes E, geralmente, os parágrafos funcionam bem. Nesse exemplo, cada parágrafo vai ser utilizado para gerar uma embedding. No script, primeiro percorro a pasta onde salvei o documento e leio a postila. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente, removo as partes que tenham menos de 10 palavras, pois é improvável que forneçam informações úteis para um contexto. Agora os embeddings podem ser gerados usando o modelo TextEmbeddingADA002. Feito isso, o próximo passo é carregar os dados, nesse caso, os vetores no Pinecone. Depois de configurado o acesso à API do Pinecone, posso criar o índice e conectar a base. Finalmente, faço o upload dos dados para o índice em lotes de 64. Para cada lote é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks, ou as partes que foram separadas em parágrafos, e deram origem aos embeddings. Em casos de uso mais complexo, os campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para upload, posso simplesmente chamar o método indexUpsert com os vetores que foram definidos. e depois que os dados forem carregados, posso realizar algumas perguntas de exemplo para encontrar os documentos mais relevantes para essa pergunta. Antes é preciso fazer o embedding da pergunta em si. Em seguida, posso realizar uma pesquisa por similaridade que compara o embedding da pergunta com todos os outros vetores no índice do Panicone. O parâmetro TopKey define quantos resultados próximos serão retornados e o parâmetro IncludeMetadata é necessário para obter nosso texto de volta com a resposta. Agora que o teste de consulta foi feito, vamos seguir para a parte final. O último passo é usar esses resultados como contexto em um prompt, nas solicitações para o modelo GPT. Aqui um bom ponto de partida para o prompt é informar que você espera respostas diretas sobre o contexto e caso a resposta não esteja presente no contexto, o modelo deve dizer apenas, eu não sei. Dessa forma, estamos impedindo o modelo de alucinar. Isso ocorre quando ele traz respostas não relacionadas à pergunta original. E como esperado, o resultado é bastante satisfatório. Se compararmos com o conteúdo da apostila, veremos que o texto foi utilizado com sucesso. Para concluir, meu palpite é que esse vai ser o caminho das aplicações que utilizarem LLMs como ChatGPT. Projetos com uma base de conhecimento sobre domínios específicos, utilizando o Vector Database como paincone. Seguido de consultas por similaridade e prompts bem projetados. Por hoje é só, eu vou ficando por aqui e a gente se vê no próximo vídeo. E aí'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738470-3f0c-40ed-a5fc-fa99a02729e2",
   "metadata": {},
   "source": [
    "## Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7d1dac-fcc4-4e7a-a6db-a26fc54c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6977fd-34e5-4ff2-9210-ec30a1b70ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f22bc86-50c5-40d5-b58a-d15cd365952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a7c3e9c-eafa-4479-b795-b1326a88c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f651b93-6029-4c0b-aa54-e56cb25fa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt_template = \"\"\"Escreva um resumo conciso com bullet points do texto abaixo:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "RESUMO CONCISO:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f9df24-1ec4-4801-a0e2-fc0479736889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **Word Embeddings**: Representação de palavras e textos como vetores numéricos, onde palavras semelhantes têm representações numéricas próximas.\\n- **Mecanismo de Busca Semântico**: Utiliza Embeddings da OpenAI para melhorar a precisão em buscas.\\n- **Processo de Criação de Embeddings**:\\n  - Instalação do pacote OpenAI e importação de bibliotecas como pandas e numpy.\\n  - Configuração da chave de API da OpenAI.\\n  - Conversão de palavras em vetores usando a função `getEmbedding`.\\n- **Cálculo de Similaridade**:\\n  - Utilização da similaridade do cosseno para medir a proximidade entre vetores.\\n  - Aplicação da função de similaridade em um DataFrame para encontrar palavras semelhantes.\\n- **Desenvolvimento de Assistentes Inteligentes**: Necessidade de entender significados e intenções em consultas, superando limitações de buscas tradicionais.\\n- **Uso de Vector Databases**: Armazenamento de embeddings de frases ou parágrafos para expandir a base de conhecimento de chatbots.\\n- **Exemplo Prático**: Vetorização de conteúdo de uma apostila, divisão em parágrafos, e uso de Pinecone para consultas por similaridade.\\n- **Resultados e Conclusão**: Implementação de prompts para evitar respostas irrelevantes e garantir que o modelo utilize o contexto corretamente.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarization chai\n",
    "llm_chain = LLMChain(llm=llm, prompt=BULLET_POINT_PROMPT)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "stuff_chain.invoke(docs)[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21b243-7d67-4048-b14c-16a7ef172be0",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "590df7d4-bc6b-4c39-a012-ee100b321764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "url = os.getenv(\"QDRANT_HOST\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"whisper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb4baff-0b46-47f4-b0aa-30df374e6454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'_id': '074d1cbc-614f-4e3d-add9-4e16da33f945', '_collection_name': 'whisper'}, page_content='Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o'),\n",
       " Document(metadata={'_id': '805ba201-cc72-4cee-bb63-c06256625f8b', '_collection_name': 'whisper'}, page_content='detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição pra API com uma palavra, como retorno eu recebo esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API, dessa forma, o que podemos fazer com o pacote Python da OpenAI é importar uma função chamada getEmbedding. Dessa forma eu só preciso passar uma string de texto, chamar a função getEmbedding e escolher uma engine, ou seja, um modelo. Como opção, temos o DaVinci, Ada, Cori, mas pra esses Embeddings de texto, vamos usar o modelo ada002. Então vou digitar a palavra caputino, e ao executar isso, a função está passando a string e convertendo essa palavra caputino pra um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores pra todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez, e criar uma nova coluna, com todos os vetores bem ao lado.'),\n",
       " Document(metadata={'_id': 'a698c1b9-8eac-4712-a998-05e708cda676', '_collection_name': 'whisper'}, page_content='Então nesse caso, posso fazer isso em lotes. Vou pegar o DataFrame e criar uma nova coluna chamada Embedding, e dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso eu utilizo a função apply, que itere em cada elemento da coluna text, e vai aplicar uma função lambda a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa uma única expressão. Nesse caso, a função getEmbedding. De maneira simples, essa linha de código aplica a função getEmbedding a cada elemento da coluna text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal Daniel, mas o que eu faço com eles? Bem, essa é a parte que a coisa fica interessante. Vou utilizar como termo de pesquisa a palavra Caputino, que geramos em Embedding inicialmente. Só lembrando que Caputino não aparece nessa lista de palavras. Então com esse vetor numérico, vou procurar uma semelhança nas representações numéricas de'),\n",
       " Document(metadata={'_id': '523874de-b9dd-4dc9-9e9a-b574ec1bf9d0', '_collection_name': 'whisper'}, page_content='No script, primeiro percorro a pasta onde salvei o documento e leio o apostilo. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente removo as partes que tenham menos de 10 palavras, pois é improvável que forneçam informações úteis para um contexto. Agora os embeddings podem ser gerados usando o modelo textEmbeddingAda002. Feito isso, o próximo passo é carregar os dados, neste caso, os vetores no Pinecone. Depois de configurado o acesso ao API do Pinecone, posso criar o índice e conectar a base. Finalmente, faço o upload dos dados para o índice em lotes de 64. Para cada lote, é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks, ou as partes que foram separadas em parágrafos e deram origem aos embeddings. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para upload, posso')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity test\n",
    "query = \"o que são word embeddings?\"\n",
    "qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899d49f-ccc5-48b9-9e82-d18b5491b5a7",
   "metadata": {},
   "source": [
    "## Resumo customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6babcabe-cb20-472d-b369-2727be219d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8c75c11-a7ba-488e-9255-fc28585dc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "    Utilize as transcrições abaixo para responder à pergunta em formato de bullet points e de forma \n",
    "    resumida. Se não souber a resposta, diga apenas que não sabe, não tente inventar ou gerar uma resposta.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Resposta resumida em bullter points:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a56d16-0316-47ba-9341-fc9d899e7dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Word Embeddings representam palavras e textos como vetores numéricos.\\n- Palavras numericamente semelhantes têm significados semelhantes.\\n- Permitem a construção de mecanismos de busca com alta precisão.\\n- O processo envolve a conversão de palavras e frases em representações numéricas.\\n- Utiliza modelos como textEmbeddingAda002 para gerar embeddings.\\n- Os dados são armazenados em plataformas como Pinecone.\\n- É possível fazer upload de dados em lotes, incluindo metadados.\\n- A função getEmbedding do pacote Python da OpenAI converte texto em vetores.\\n- Vetores podem ser gerados em massa para várias palavras em um DataFrame.\\n- Embeddings são utilizados para encontrar semelhanças em representações numéricas.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Resuma word embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f9124-2360-4bcc-9f4b-ae5dd4ba3e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
