{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f497904-26e5-4f48-b6f0-04198278a1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf48350-e6a6-4154-b8dd-37fd061183ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    filename = 'infoslack.mp4'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=iw2TeYESnTk\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e7cbdb-312f-4dd1-ba9a-7097687d6d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=iw2TeYESnTk\n",
      "[youtube] iw2TeYESnTk: Downloading webpage\n",
      "[youtube] iw2TeYESnTk: Downloading ios player API JSON\n",
      "[youtube] iw2TeYESnTk: Downloading m3u8 information\n",
      "[info] iw2TeYESnTk: Downloading 1 format(s): 251\n",
      "[download] Destination: infoslack-audio\n",
      "[download] 100% of   10.89MiB in 00:00:01 at 7.36MiB/s     \n",
      "[ExtractAudio] Destination: infoslack-audio.mp3\n",
      "Deleting original file infoslack-audio (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "def download_audio(url):\n",
    "    \n",
    "    filename = 'infoslack-audio'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio',\n",
    "        'postprocessors':[{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '128',\n",
    "        }],\n",
    "        'outtmpl': filename,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "        \n",
    "download_audio(\"https://www.youtube.com/watch?v=iw2TeYESnTk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b271f381-784a-4be8-8c15-e25163219cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 9s, sys: 1min 47s, total: 7min 57s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper local\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"infoslack-audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38496097-9ee6-4c65-b073-02e8018ce101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O orden bearings são a forma de representar palavras e textos como vetores numericos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converse uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como mecanismo de busca, com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semânticos usando emberes da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, convertem númeres para realizar a classificação, da textão de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural. Bem, eu preparei esse exemplo sobre emberes de palavras e agora vamos ver como tudo isso funciona. A matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o pacote da OpenAI, como já fiz em outros vídeos. O próximo passo aqui é importar os pacotes Python que vão ser necessários. Então, além de importar o pacote da OpenAI, vou precisar do Pandas e NumPy. Aqui, eu preciso configurar a chave de API da OpenAI. Vou gerar um chave de API nova, assim você pode relembrar onde como fazer. Para garantir uma compreensão muito clara desse conceito e como ele funciona, vou começar com uma lista muito simples de palavras. Esse é o arquivo CSV de palavras. Temos algumas palavras como Batata Frita, refrigerante, hamburgues, banana, café e coisas assim. Agora, vou carregar esse arquivo CSV em um Datafream do Pandas e vou chamar esse Datafream de IDF. A próxima coisa que faremos é calcular os emberes das palavras. Mas, o que é isso? Isso significa converter essas palavras em vetores. E então, o que eu vou fazer aqui é usar o API para essa tarefa. E, se você olhar na documentação da OpenAI, tem uma sessão inteira sobre emberes com mais detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição para API com uma palavra, como retorno o recebo, esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API dessa forma, o que podemos fazer com o pacote Python da OpenAI, importar uma função chamada GetInBerry. Dessa forma, eu só preciso passar um string de texto, chamar a função GetInBerry e escolher o Engine, ou seja, um modelo. Com uma opção, temos o da 20, aada, currê, mais pra esses emberes de texto. Vamos usar um modelo a 002. Então, vou digitar a palavra caputino. E, ao executar isso, a função está passando a string e convertendo essa palavra caputino para um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores para todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez. E criar uma nova coluna, com todos os vetores bem ao lado. Então, nesse caso, posso fazer isso em lots. Vou pegar o DataFrame e criar uma nova coluna chamada em Berry. E, dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso, utiliza a função Apply, queitere em cada elemento da coluna Text, e vai aplicar uma função Lambda, a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa um a única expressão, nesse caso, a função GetInBerry. De maneira simples, essa linha de código aplica a função GetInBerry, a cada elemento da coluna Text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal, Daniel, mas o que eu faço com eles? Bem, essa é a parte que é coisa que fique interessante. Vou utilizar como termo de pesquisa a palavra caputino, que geramos em Berry inicialmente. Só lembrando que caputino não aparece nessa lista de palavras. Então, com esse vetor numérico, vou procurar uma semelhança. Nas representações numéricas de todas essas outras palavras que temos armazenadas no DataFrame. Ou seja, eu quero descobrir quais vetores estão mais próximos do vetor da palavra caputino. Em outras palavras, vamos fazer uma pesquisa por similaridade. E pra fazer isso, o modelo faz um cálculo da medida de similaridade entre dois vetores, chamada Cossine Similarity, ou similaridade do Cosseno. Essa é uma fórmula matemática que calcula a similaridade entre duas sequências de números, e é descrita por essa coação. A similaridade de Cosseno é o vitil da dividida no produto escalado dos dois vetores pelas magnitudes desses vetores. Essa notação pode parecer complicada, ou muito simples, dependendo do seu nível de matemática, mas eu garanto que é apenas multiplicação. Bom, explicando de forma bem simples esse termo superior aqui, é um produto escalar, que é o vitido somando os produtos dos elementos correspondentes dos vetores. Traduzindo, o objetivo é apenas multiplicar cada um dos termos nos dois vetores juntos. Suponha que você tem essa equação aqui com dois vetores, onde o primeiro vetor V1 são os números 1, 2 e 3. E o segundo vetor, ou V2, é 4, 5 e 6. Então, o cálculo ficaria 1 vezes 4 mais 2 vezes 5 mais 3 vezes 6. E como resultado, temos o produto escalar. Ou em Python, podemos usar ou não para fazer isso. Obtendo o mesmo resultado, 32. E então, na parte inferior da equação, precisamos calcular a magnitude. A magnitude de um vetor é um valor escalar que representa o tamanho ou o comprimento do vetor. É calculado como a raiz quadrada da soma dos quadrados dos elementos do vetor. Ou seja, 1 ao quadrado, mais 2 ao quadrado, mais 3 ao quadrado, que nesse caso dá 14. Eu pego a raiz quadrada disso, ou seja, 3.74. Faça essa mesma conta no V2, multiplique os dois, e tem a magnitude. Agora, eu só preciso dividir o produto escalar pela magnitude para obter a similaridade. Que nesse caso, é de 0.97, entre essas duas sequências de números. No final, tudo é apenas uma soma de várias multiplicações. Se pode até pegar esses números aqui, plotar em um vetor 3D. Se eu colocar 1, 2 e 3 aqui, como um vetor 4, 5, 6, e desenhar, você pode ver que eles estão bem próximos. Voltando às palavras, o que estamos fazendo aqui, é pegando uma representação vetorial de todas essas palavras diferentes e ver qual próximas elas estão no espaço. Claro que é difícil realizar esse cálculo manualmente com essa quantidade de números, mas, felizmente, existe uma função integrada na própria OPNI, chamada a coçade simulária, que percorrerá todos esses números e executará esse cálculo. Então, agora podemos simplesmente classificar todos esses diferentes vetores no espaço e descobrir qual o vetor está mais próximo de capotino. Que vamos fazer aqui, é pegar essa função de simulária e idade do coçeno e aplicá-la a cada vetor que temos no nosso data frame. Ou seja, vamos verificar a distância entre cada um desses vetores e o vetor para capotino. Em tensa que o estamos nesse cálculo e armazenamos o resultado em uma nova coluna chamada a simularidades. Por fim, ordenamos em ordem de acrescente pela coluna simularidades. Como resultado, temos as palavras mais similares a capotina. Nesse caso, expresso o Moca Laté e Café, o que faz total sentido. Se você ficou interessado nesse assunto e quer saber mais sobre como embarees funcionam e como esses modelos entendem o significado de palavras, deixa aí nos comentários. Agora penso o seguinte, você está desenvolvendo um assistente inteligente simular o chat gpt com foco em um domínio específico dentro de um negócio. Pois bem, esses modelos precisam saber o máximo possível sobre as suas operações. Mas a maioria das empresas possui tanta informação que coloca tudo dentro de um só contexto, em uma única só licitação enviável. Um outro problema é que nas conversas, os usuários geralmente não fazem perguntas baseadas em palavras chave, específicas como fazem no Google. Em vez disso, fazem perguntas que expressam um significado ou uma intenção específica. Ou seja, algoritmos de busca tradicional estariam dificuldades com esse tipo de consulta. E aqui, que soluções como painicone entram em cena. Imagina poder fazer embarees de frases ou até paraagra-vos inteiros. De armazenar isso em um banco de dados. Banco de dados de vetores ou vector databases Permite que você expanda de maneira absurda a base de conhecimento do seu chatbot com os seus dados personalizados. Vamos ver isso na prática. Nesse exemplo, vou usar como base de conhecimento essa pochila aberta de Python da caelo. Aí deve autorizar todo esse conteúdo para fazer consultas por similaridade e utilizar o resultado como contexto nos prontes. Depois de importar os pacotes necessários, preciso garantir que o documento não é cedo limite de tokens. Pra isso, a estratégia é dividido o documento em partes menores. É uma boa prática de vídeos documentos em partes semanticamente relevantes. E geralmente, os parágrafos funcionam bem. Nesse exemplo, cada parágrafo vai ser utilizado para gerar uma embaring. No script, primeiro percorro a parte da onde salver o documento e leu a pochila. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente, removo as partes que tenham menos de 10 palavras, pois, improvável que forneça informações úteis para um contexto. Agora os embérios podem ser gerados usando o modelo Text Embaring Addas 002. Feito isso, o próximo passo é carregar os lados. Nesse caso, os vetores no panicônia. Depois de configurado o acesso a API do panicônia, possa criar o índice e conectar a base. Finalmente, faço o apilô de dos dados pro índice em lots de 64. Para cada lote é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks ou as partes que foram separadas em parágrafos e deram origem aos embérios. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para o pelot, possa simplesmente chamar o método index absurd com os vetores que foram definidos. E depois que os dados forem carregados, posso realizar algumas perguntas de exemplo. Para encontrar os documentos mais relevantes para essa pergunta. Antes de fazer o embaring, dá pergunta em si. Nesse seguida, posso realizar uma pesquisa possibilidade que comparam o embaring da pergunta com todos os outros vetores no índice do panicón. O parâmetro topkey define quantos resultados próximos serão retornados. E o parâmetro, inclui o métodeita, é necessário para obter nosso texto e volta com a resposta. Agora que o teste de consulta foi feito, vamos seguir para a parte final. O último passo é usar esses resultados como contexto em um prąpe. Nae sólos citações para o modelo GPT. Aqui é um bom ponto de partida para o prąpe, é informar que você espera respostas diretas sobre o contexto e caso a resposta não esteja presente no contexto, o modelo deve dizer apenas, eu não sei. Deça forma estamos impedindo o modelo de alucinar. Isso ocorre quando ele traz respostas não relacionadas a perguntar original. E como esperado, o resultado é bastante satisfatório. Se compararmos com o conteúdo da apostil, veremos que o texto foi utilizado com sucesso. Para concluir, meu palpite o que esse vai ser o caminho das aplicações que utilizarem a Lele EMS como chatRPT. Projetos com uma base de conhecimento sobre domínios específicos utilizando o Vector Data Base como panicónio. Seguido de consultas por similaridade e prąpe de bem projetados. Por hoje é só, eu vou ficando por aqui e a gente se vê no próximo vídeo.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6687192-b34c-4de1-888d-d00f5f59453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 270 ms, total: 1.39 s\n",
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper API\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"infoslack-audio.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")\n",
    "\n",
    "result = transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27549fef-2001-4a38-953a-4501454544b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text.txt', 'w') as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f5489ad-7548-4b04-b033-9a58acebb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.9 ms, sys: 42.6 ms, total: 122 ms\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# whisper API Groq\n",
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "resp = client.audio.transcriptions.create(\n",
    "    model=\"whisper-large-v3\",\n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738470-3f0c-40ed-a5fc-fa99a02729e2",
   "metadata": {},
   "source": [
    "## Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7d1dac-fcc4-4e7a-a6db-a26fc54c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f6977fd-34e5-4ff2-9210-ec30a1b70ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f22bc86-50c5-40d5-b58a-d15cd365952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c3e9c-eafa-4479-b795-b1326a88c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f651b93-6029-4c0b-aa54-e56cb25fa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt_template = \"\"\"Escreva um resumo conciso com bullet points do texto abaixo:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "RESUMO CONCISO:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc32c0de-97d7-4330-b48d-79e71b5270ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Word Embeddings**: Representação de palavras e textos como vetores numéricos, onde palavras semelhantes têm representações numéricas próximas.\n",
      "- **Mecanismo de Busca Semântico**: Utiliza Embeddings da OpenAI para melhorar a precisão em buscas.\n",
      "- **Processo de Criação de Embeddings**:\n",
      "  - Instalação do pacote OpenAI e importação de bibliotecas como pandas e numpy.\n",
      "  - Configuração da chave de API da OpenAI.\n",
      "  - Conversão de palavras em vetores usando a função `getEmbedding`.\n",
      "- **Cálculo de Similaridade**:\n",
      "  - Utilização da similaridade do cosseno para medir a proximidade entre vetores.\n",
      "  - Implementação de cálculos de produto escalar e magnitudes dos vetores.\n",
      "- **Aplicação Prática**:\n",
      "  - Pesquisa de similaridade usando um vetor de uma palavra (ex: \"Caputino\") para encontrar palavras relacionadas.\n",
      "  - Classificação de palavras com base na similaridade.\n",
      "- **Desenvolvimento de Assistentes Inteligentes**:\n",
      "  - Necessidade de entender significados e intenções em consultas, superando\n",
      "limitações de buscas tradicionais.\n",
      "  - Uso de bancos de dados de vetores (vector databases) para expandir a base de conhecimento de chatbots.\n",
      "- **Exemplo Prático com Pinecone**:\n",
      "  - Vetorização de documentos e armazenamento em um banco de dados.\n",
      "  - Realização de consultas por similaridade e uso de resultados como contexto em prompts para modelos GPT.\n",
      "- **Conclusão**: Expectativa de que aplicações futuras utilizem LLMs com bases de conhecimento específicas e consultas por similaridade.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21b243-7d67-4048-b14c-16a7ef172be0",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "590df7d4-bc6b-4c39-a012-ee100b321764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "url = os.getenv(\"QDRANT_HOST\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"whisper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb4baff-0b46-47f4-b0aa-30df374e6454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'_id': '573b6326-974d-41ef-8771-e2e778d41d49', '_collection_name': 'whisper'}, page_content='Word Embeddings são uma forma de representar palavras e textos como vetores numéricos. E a coisa mais interessante sobre esse conceito é que quando você pega palavras ou frases e converte em uma representação numérica, palavras que são numericamente semelhantes são semelhantes em significado também. E isso nos permite construir coisas como um mecanismo de busca com um nível de precisão interessante. Nesse vídeo vamos explorar um sistema de busca semântico usando Embeddings da OpenAI. Então, como funciona esse mecanismo de busca? Como pegar essas palavras e frases, converter em números para realizar classificação, detecção de anomalias, agrupamento, enfim, todas essas tarefas legais de linguagem natural? Bem, eu preparei esse exemplo sobre Embeddings de palavras e agora vamos ver como tudo isso funciona. Há matemática por trás, de uma maneira que você possa entender e aplicar. Eu vou deixar um link desse projeto na descrição. Bem, a primeira coisa que precisamos fazer é instalar o'),\n",
       " Document(metadata={'_id': '9312e5a8-37be-4579-83bf-8ec079dcece9', '_collection_name': 'whisper'}, page_content='detalhes e exemplos. Mas vamos continuar. Ao enviar uma requisição pra API com uma palavra, como retorno eu recebo esse grande vetor com uma série de números. Para o nosso exemplo, em vez de utilizar essa requisição de API, dessa forma, o que podemos fazer com o pacote Python da OpenAI é importar uma função chamada getEmbedding. Dessa forma eu só preciso passar uma string de texto, chamar a função getEmbedding e escolher uma engine, ou seja, um modelo. Como opção, temos o DaVinci, Ada, Cori, mas pra esses Embeddings de texto, vamos usar o modelo ada002. Então vou digitar a palavra caputino, e ao executar isso, a função está passando a string e convertendo essa palavra caputino pra um vetor. E temos essa quantidade absurda de números que o modelo retorna. O que vou fazer agora é a mesma coisa no DataFrame, gerando vetores pra todas essas palavras. Aqui eu tenho um total de 26 palavras, e quero converter todas elas de uma vez, e criar uma nova coluna, com todos os vetores bem ao lado.'),\n",
       " Document(metadata={'_id': '091898d5-4391-4dd4-92b8-a3958cd2ac5c', '_collection_name': 'whisper'}, page_content='Então nesse caso, posso fazer isso em lotes. Vou pegar o DataFrame e criar uma nova coluna chamada Embedding, e dentro dessa nova coluna, o que vou fazer é armazenar o resultado dessa operação. Pra isso eu utilizo a função apply, que itere em cada elemento da coluna text, e vai aplicar uma função lambda a cada um desses elementos. Lambda é uma função anônima que recebe um único argumento, e executa uma única expressão. Nesse caso, a função getEmbedding. De maneira simples, essa linha de código aplica a função getEmbedding a cada elemento da coluna text. Como resultado, pra cada palavra nesse DataFrame, tem uma grande lista de números associados. Legal Daniel, mas o que eu faço com eles? Bem, essa é a parte que a coisa fica interessante. Vou utilizar como termo de pesquisa a palavra Caputino, que geramos em Embedding inicialmente. Só lembrando que Caputino não aparece nessa lista de palavras. Então com esse vetor numérico, vou procurar uma semelhança nas representações numéricas de'),\n",
       " Document(metadata={'_id': '60f0cc8c-5c29-408d-8e7b-66a3b2037b16', '_collection_name': 'whisper'}, page_content='No script, primeiro percorro a pasta onde salvei o documento e leio o apostilo. Em seguida, é feita a extração dos parágrafos como partes separadas. E na sequência, são armazenados em uma lista. Finalmente removo as partes que tenham menos de 10 palavras, pois é improvável que forneçam informações úteis para um contexto. Agora os embeddings podem ser gerados usando o modelo textEmbeddingAda002. Feito isso, o próximo passo é carregar os dados, neste caso, os vetores no Pinecone. Depois de configurado o acesso ao API do Pinecone, posso criar o índice e conectar a base. Finalmente, faço o upload dos dados para o índice em lotes de 64. Para cada lote, é necessário os metadados. Nesse exemplo, os metadados serão simplesmente os chunks, ou as partes que foram separadas em parágrafos e deram origem aos embeddings. Em casos de uso mais complexos, campos adicionais, como a quantidade de tokens que o texto vai consumir, podem ser úteis. Assim que o lote estiver pronto para upload, posso')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity test\n",
    "query = \"o que são word embeddings?\"\n",
    "qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899d49f-ccc5-48b9-9e82-d18b5491b5a7",
   "metadata": {},
   "source": [
    "## Resumo customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6babcabe-cb20-472d-b369-2727be219d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c75c11-a7ba-488e-9255-fc28585dc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "    Utilize as transcrições abaixo para responder à pergunta em formato de bullet points e de forma \n",
    "    resumida. Se não souber a resposta, diga apenas que não sabe, não tente inventar ou gerar uma resposta.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Resposta resumida em bullter points:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d79644-1b1f-4e74-8375-14a1b05786ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/infoslack/Projects/yt_br_llm/envs/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'- Word Embeddings representam palavras e textos como vetores numéricos.\\n- Palavras numericamente semelhantes têm significados semelhantes.\\n- Permitem a construção de mecanismos de busca com alta precisão.\\n- O processo envolve a conversão de palavras e frases em representações numéricas.\\n- Utiliza modelos como textEmbeddingAda002 para gerar embeddings.\\n- Os dados são armazenados em plataformas como Pinecone.\\n- É possível fazer upload de dados em lotes, incluindo metadados.\\n- A API da OpenAI pode ser utilizada para gerar embeddings a partir de strings de texto.\\n- Exemplos práticos incluem a conversão de várias palavras em vetores simultaneamente.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Resuma word embeddings\"\n",
    "docs = qdrant.similarity_search(query, k=3)\n",
    "context = docs[0].page_content + docs[1].page_content + docs[2].page_content\n",
    "res = LLMChain(prompt=prompt, llm=llm)\n",
    "res.run(query=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566de9b-df3d-4b43-be43-f5909a733a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
