{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f431cc9-c588-48df-9e56-019352642050",
   "metadata": {},
   "source": [
    "### install libraries\n",
    "```bash\n",
    "pip install -U \\\n",
    "    openai \\\n",
    "    langchain \\\n",
    "    langchain_openai \\\n",
    "    langchain-community \\\n",
    "    python-dotenv \\\n",
    "    datasets \\\n",
    "    qdrant-client \\\n",
    "    tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54e38a3-161d-4ef1-b82e-41677474380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b1049b-0fc2-4549-bf37-50ead6cdc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6454b6c-3426-4135-ae7d-9c8b57c07d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente útil que responde perguntas.\"),\n",
    "    HumanMessage(content=\"Olá Bot, como você está hoje?\"),\n",
    "    AIMessage(content=\"Estou bem, obrigado. Como posso ajudar?\"),\n",
    "    HumanMessage(content=\"Gostaria de entender o que é machine learning.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aace1fe-d249-48f3-b0ea-d82a9016379e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e técnicas que permitem que os computadores aprendam a partir de dados e façam previsões ou tomem decisões sem serem explicitamente programados para isso. Em essência, o objetivo do machine learning é permitir que os computadores aprendam automaticamente e melhorem seu desempenho ao longo do tempo, com base na experiência adquirida através dos dados. Isso é feito através do treinamento de modelos de machine learning com conjuntos de dados e algoritmos apropriados, para que possam aprender padrões e fazer previsões ou tomar decisões com base nesses padrões.', response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 65, 'total_tokens': 221}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f1b25202-38d9-4415-ade3-b71e50c53f73-0', usage_metadata={'input_tokens': 65, 'output_tokens': 156, 'total_tokens': 221})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ed228a-d321-49e6-bd4d-f6bc7750e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e técnicas que permitem que os computadores aprendam a partir de dados e façam previsões ou tomem decisões sem serem explicitamente programados para isso. Em essência, o objetivo do machine learning é permitir que os computadores aprendam automaticamente e melhorem seu desempenho ao longo do tempo, com base na experiência adquirida através dos dados. Isso é feito através do treinamento de modelos de machine learning com conjuntos de dados e algoritmos apropriados, para que possam aprender padrões e fazer previsões ou tomar decisões com base nesses padrões.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2cedc5-9e3b-42bd-a108-f6aaba2f46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cada1b-11ec-40c3-a9d7-d03186ae8cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Você é um assistente útil que responde perguntas.'),\n",
       " HumanMessage(content='Olá Bot, como você está hoje?'),\n",
       " AIMessage(content='Estou bem, obrigado. Como posso ajudar?'),\n",
       " HumanMessage(content='Gostaria de entender o que é machine learning.'),\n",
       " AIMessage(content='Machine learning é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e técnicas que permitem que os computadores aprendam a partir de dados e façam previsões ou tomem decisões sem serem explicitamente programados para isso. Em essência, o objetivo do machine learning é permitir que os computadores aprendam automaticamente e melhorem seu desempenho ao longo do tempo, com base na experiência adquirida através dos dados. Isso é feito através do treinamento de modelos de machine learning com conjuntos de dados e algoritmos apropriados, para que possam aprender padrões e fazer previsões ou tomar decisões com base nesses padrões.', response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 65, 'total_tokens': 221}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f1b25202-38d9-4415-ade3-b71e50c53f73-0', usage_metadata={'input_tokens': 65, 'output_tokens': 156, 'total_tokens': 221})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9cd0b5-abc7-434f-8d91-91da3719a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"Qual é a diferença entre supervisionado e não supervisionado?\"\n",
    ")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5757334d-e734-4245-a358-d91dbcb15de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No aprendizado de máquina supervisionado, os algoritmos são treinados em um conjunto de dados rotulados, ou seja, cada exemplo de entrada está associado a uma saída desejada. O objetivo é que o algoritmo aprenda a mapear corretamente as entradas para as saídas, com base nos rótulos fornecidos no conjunto de treinamento. Por exemplo, em um problema de classificação, o conjunto de dados de treinamento incluiria exemplos de entrada juntamente com suas classes correspondentes.\n",
      "\n",
      "Já no aprendizado de máquina não supervisionado, os algoritmos são treinados em conjuntos de dados não rotulados, ou seja, os dados de entrada não têm rótulos associados. O objetivo é encontrar padrões, estruturas ou relações nos dados sem a orientação de saídas conhecidas. Por exemplo, em um problema de clusterização, o algoritmo tentaria agrupar os dados em clusters ou grupos com base em similaridades entre eles, sem a necessidade de rótulos específicos.\n",
      "\n",
      "Em resumo, a diferença fundamental entre aprendizado supervisionado e não supervisionado está na presença ou ausência de rótulos nos dados de treinamento, o que influencia a abordagem e os tipos de problemas que podem ser abordados por cada tipo de aprendizado de máquina.\n"
     ]
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16610f-26d3-4b10-81da-fd06efee89ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f1be5-27fe-4ca0-9857-9f7bae2579bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d28b9-c4fb-48dd-95b0-4afecfc0764b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690ea633-c5d5-47d7-b8de-546ba6f2ec42",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d7366c-8a9e-4154-958e-af7c28b09950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"O que tem de tão especial no Mistral 7B?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a6dbe6-68e3-4e8b-a4f1-9eba87f05a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é uma linha de vinhos produzida pela vinícola chilena Viña Montes. Essa linha é conhecida por sua qualidade e sabor distintivo, resultante da combinação de uvas cuidadosamente selecionadas e técnicas de vinificação sofisticadas. O Mistral 7B destaca-se por sua riqueza de aromas e sabores, resultando em vinhos complexos e elegantes. Além disso, a Viña Montes é uma vinícola premiada, reconhecida internacionalmente pela excelência de seus vinhos, o que contribui para a reputação especial do Mistral 7B. Se você é um apreciador de vinhos, vale a pena experimentar o Mistral 7B para descobrir por si mesmo o que o torna tão especial.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85199c22-35ef-4e1f-afde-1ffe72ce568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Você pode me falar sobre o LLMChain no LangChain?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef326306-e957-4c2c-9578-536e98b3cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peço desculpas, mas não possuo informações específicas sobre o \"LLMChain\" no \"LangChain\". Se você puder fornecer mais detalhes ou contexto sobre o que você gostaria de saber, ficarei feliz em tentar ajudar da melhor forma possível.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89627e4-13ea-4baf-9347-94915405c991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73381e-6162-490c-a867-a7bc6e78da8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174190b8-52ac-48d3-ace2-8f1092566f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43539d31-36b2-4da6-89d1-1d97b3ad79c0",
   "metadata": {},
   "source": [
    "### Context injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a4e84e-5b93-4569-bb40-ff3559d927c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6614e687-9ef7-4b38-926c-35b8e557b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Você pode me falar sobre o LLMChain no LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
    "\n",
    "Contexto:\n",
    "{source_knowledge}\n",
    "\n",
    "Pergunta: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c93aa9bb-c3db-408c-9068-b447a8171364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O LLMChain no LangChain é um tipo comum de cadeia (chain) que faz parte do framework LangChain, que é utilizado para desenvolver aplicações alimentadas por modelos de linguagem. O LLMChain é composto por um PromptTemplate, um modelo (seja um LLM ou um ChatModel) e um analisador de saída opcional. Essa cadeia recebe várias variáveis de entrada, utiliza o PromptTemplate para formatá-las em um prompt, passa esse prompt para o modelo e, por fim, utiliza o OutputParser (se fornecido) para analisar a saída do LLM em um formato final.\n",
      "\n",
      "O LangChain é um framework que visa possibilitar o desenvolvimento de aplicações que vão além de simplesmente chamar um modelo de linguagem via API. Ele busca criar aplicações que sejam conscientes dos dados, conectando o modelo de linguagem a outras fontes de dados, e que sejam agentes capazes de interagir com o ambiente ao seu redor. Portanto, o LangChain é projetado com o objetivo de permitir o desenvolvimento de aplicações com essas características, e o LLMChain desempenha um papel importante dentro desse framework.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3147887-edd7-4bdd-b704-a141a554fc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c73def-96ba-479e-b5d0-c933cf7ccac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211ee60-7f05-4439-95e1-a7fde4890d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f379c6b2-9f0a-419b-877b-f3f4d2af2555",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a833de3-1bb3-4839-afac-ba798d8454cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36cc8b6-cc3f-4568-8314-0612a947dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4115be5-86ec-45e6-88a9-8caca2765487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ce1a7-934b-4ecc-a7eb-f562e969c101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e57b12-ec66-4bae-af23-f8a4bdc0d2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb6f84ad-316f-402e-a385-728ec11ae278",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32fb1fa1-f498-42ff-8a0d-5b02602ba6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "444f9b75-8668-47fb-a83f-069e1038d25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2310.06825',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src',\n",
       " 'id': '2310.06825',\n",
       " 'title': 'Mistral 7B',\n",
       " 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.',\n",
       " 'source': 'http://arxiv.org/pdf/2310.06825',\n",
       " 'authors': ['Albert Q. Jiang',\n",
       "  'Alexandre Sablayrolles',\n",
       "  'Arthur Mensch',\n",
       "  'Chris Bamford',\n",
       "  'Devendra Singh Chaplot',\n",
       "  'Diego de las Casas',\n",
       "  'Florian Bressand',\n",
       "  'Gianna Lengyel',\n",
       "  'Guillaume Lample',\n",
       "  'Lucile Saulnier',\n",
       "  'Lélio Renard Lavaud',\n",
       "  'Marie-Anne Lachaux',\n",
       "  'Pierre Stock',\n",
       "  'Teven Le Scao',\n",
       "  'Thibaut Lavril',\n",
       "  'Thomas Wang',\n",
       "  'Timothée Lacroix',\n",
       "  'William El Sayed'],\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
       " 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20231010',\n",
       " 'updated': '20231010',\n",
       " 'references': [{'id': '1808.07036'},\n",
       "  {'id': '1809.02789'},\n",
       "  {'id': '1904.10509'},\n",
       "  {'id': '2302.13971'},\n",
       "  {'id': '2009.03300'},\n",
       "  {'id': '2305.13245'},\n",
       "  {'id': '1904.09728'},\n",
       "  {'id': '1803.05457'},\n",
       "  {'id': '2103.03874'},\n",
       "  {'id': '1905.07830'},\n",
       "  {'id': '2308.12950'},\n",
       "  {'id': '2210.09261'},\n",
       "  {'id': '2310.06825'},\n",
       "  {'id': '2307.09288'},\n",
       "  {'id': '2304.06364'},\n",
       "  {'id': '1905.10044'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2108.07732'},\n",
       "  {'id': '2107.03374'},\n",
       "  {'id': '1811.00937'},\n",
       "  {'id': '2004.05150'},\n",
       "  {'id': '1705.03551'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05a7d51-4025-497d-bc55-51fa94614ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>0</td>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>1</td>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>2</td>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>3</td>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>4</td>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi chunk-id                                              chunk  \\\n",
       "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
       "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
       "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
       "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
       "\n",
       "           id       title                                            summary  \\\n",
       "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2310.06825   \n",
       "1  http://arxiv.org/pdf/2310.06825   \n",
       "2  http://arxiv.org/pdf/2310.06825   \n",
       "3  http://arxiv.org/pdf/2310.06825   \n",
       "4  http://arxiv.org/pdf/2310.06825   \n",
       "\n",
       "                                             authors             categories  \\\n",
       "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  Models and code are available at\\n  https://mi...        None   \n",
       "1  Models and code are available at\\n  https://mi...        None   \n",
       "2  Models and code are available at\\n  https://mi...        None   \n",
       "3  Models and code are available at\\n  https://mi...        None   \n",
       "4  Models and code are available at\\n  https://mi...        None   \n",
       "\n",
       "  primary_category published   updated  \\\n",
       "0            cs.CL  20231010  20231010   \n",
       "1            cs.CL  20231010  20231010   \n",
       "2            cs.CL  20231010  20231010   \n",
       "3            cs.CL  20231010  20231010   \n",
       "4            cs.CL  20231010  20231010   \n",
       "\n",
       "                                          references  \n",
       "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d48e8ca9-2c76-4c64-9d5d-391ddba5a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  automated benchmarks. Our models are released ...   \n",
       "2  GQA significantly accelerates the inference sp...   \n",
       "3  Mistral 7B takes a significant step in balanci...   \n",
       "4  parameters of the architecture are summarized ...   \n",
       "\n",
       "                            source  \n",
       "0  http://arxiv.org/pdf/2310.06825  \n",
       "1  http://arxiv.org/pdf/2310.06825  \n",
       "2  http://arxiv.org/pdf/2310.06825  \n",
       "3  http://arxiv.org/pdf/2310.06825  \n",
       "4  http://arxiv.org/pdf/2310.06825  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data[['chunk', 'source']]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c88a3fb-0d01-40bb-a488-b6d419024d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2279c98-f25f-49e9-9c57-458c4da9f31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c057041-1227-4ea5-82e6-63174c8a73a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "158f1fa6-b9d6-4063-8706-44c7f17481dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0569ba2e-acc3-4e63-8df7-3bdf6f7db0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"chatbot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8501c9a7-6781-46c5-86ac-453ffc9d2ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'c79e41f564c44208871be12ce47fad5a', '_collection_name': 'chatbot'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '5cb51e702f2442468e5219d45bceb439', '_collection_name': 'chatbot'}, page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'a6a70d0dec3244eda2ac8e3897adeb86', '_collection_name': 'chatbot'}, page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"O que tem de tão especial no Mistral 7B?\"\n",
    "qdrant.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6acfdfdd-2ed0-4d3e-886b-fa816cd70238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
    "\n",
    "    Contexto:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Pergunta: {query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8526b5e0-abdc-4f21-aefc-ecd3527bd219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use o contexto abaixo para responder à pergunta.\n",
      "\n",
      "    Contexto:\n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
      "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
      "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
      "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
      "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
      "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
      "large language models efficient. Through our work, our aim is to help the community create more\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "1 Introduction\n",
      "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
      "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
      "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
      "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
      "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
      "a carefully designed language model can deliver high performance while maintaining an efficient\n",
      "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
      "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
      "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
      "without sacrificing performance on non-code related benchmarks.\n",
      "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "\n",
      "    Pergunta: O que tem de tão especial no Mistral 7B?\n"
     ]
    }
   ],
   "source": [
    "print(custom_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6b3f8a1-2a52-4a56-95a9-9bb79e4f234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem com 7 bilhões de parâmetros projetado para oferecer desempenho superior e eficiência. Ele se destaca por superar o melhor modelo aberto de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados, bem como o melhor modelo lançado de 34 bilhões de parâmetros (Llama 1) em raciocínio, matemática e geração de código. O Mistral 7B utiliza atenção de consulta agrupada (GQA) para acelerar a inferência, em conjunto com atenção de janela deslizante (SWA) para lidar efetivamente com sequências de comprimento arbitrário com um custo de inferência reduzido. Além disso, o modelo inclui uma versão fina-tunada para seguir instruções, chamada Mistral 7B - Instruct, que supera o modelo de chat Llama 2 de 13 bilhões de parâmetros em benchmarks humanos e automatizados. O Mistral 7B é lançado sob a licença Apache 2.0 e disponibiliza implementações de referência para facilitar a implantação local ou em plataformas de nuvem como AWS, GCP ou Azure. O modelo é projetado para facilitar o ajuste fino em uma variedade de tarefas e demonstra adaptabilidade e desempenho superiores. Em resumo, o Mistral 7B se destaca por seu desempenho aprimorado, eficiência, facilidade de ajuste fino e implementações prontas para facilitar sua utilização e integração.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366ff2d-2f0b-4ef4-bede-33c5f26758e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8506158-0046-4c69-bf43-505ac0e97db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3e165-c006-404b-86f0-eccf8b157a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29ed7a06-c5e2-491f-81d5-5b5b04e8ba40",
   "metadata": {},
   "source": [
    "## Groq\n",
    "\n",
    "```bash\n",
    "pip install langchain-groq\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db0b946f-b753-4238-8279-d5d75dc6bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=0, model_name=\"llama-3.1-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c487f1c-a62b-4621-b83d-c78431c34de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem de 7 bilhões de parâmetros projetado para desempenho superior e eficiência. Ele supera o melhor modelo de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados e o melhor modelo de 34 bilhões de parâmetros (Llama 1) em raciocínio, matemática e geração de código.\n",
      "\n",
      "O que torna o Mistral 7B especial é a combinação de duas tecnologias:\n",
      "\n",
      "1. **Grouped-Query Attention (GQA)**: essa tecnologia acelera significativamente a velocidade de inferência e reduz a necessidade de memória durante a decodificação, permitindo que o modelo processe mais dados em paralelo.\n",
      "2. **Sliding Window Attention (SWA)**: essa tecnologia permite que o modelo processe sequências de comprimento arbitrário com um custo computacional reduzido, o que é uma limitação comum em modelos de linguagem.\n",
      "\n",
      "Essas duas tecnologias trabalham juntas para melhorar o desempenho e a eficiência do Mistral 7B, tornando-o um modelo de linguagem mais poderoso e eficiente. Além disso, o Mistral 7B é projetado para ser facilmente personalizável para uma variedade de tarefas e é liberado sob a licença Apache 2.0.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dbf8d-0758-4df2-920e-8f5eab3f87b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
